{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec498da2",
   "metadata": {},
   "source": [
    "# Image Registration of Planification CT with Synthetic CT \n",
    "ESCUELA DE INGENIERIA DE FUENLABRADA\n",
    "\n",
    "**Biomedical Engineering Bachelor Thesis**\n",
    "\n",
    "**Author:** Javier Alfonso Villoldo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93122dcc",
   "metadata": {},
   "source": [
    "*Automatic 3D registration pipeline using ITK-elastix Registration Toolkit (SITK) is written in Python language (version 3.10.9, 64 bit architecture).*\n",
    "\n",
    "The code provides a comprehensive registration pipeline, including all necessary preprocessing and postprocessing steps.\n",
    "The code includes a comparison between two registration methods, a tailored registration pipeline and a Deep-Learning (DL) based approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442720e",
   "metadata": {},
   "source": [
    "## Previous Concers\n",
    "\n",
    "In recent years, on-board cone-beam computed tomography (CBCT) systems have been adopted in both photon and proton therapy centers for accurate pre-treatment patient alignment. Beyond positioning, CBCT images also reveal changes in patient anatomy between treatment sessions. Accurate stopping power ratios (SPR) derived from CT numbers are crucial for precise proton dose calculations, but the relationship between CT numbers and SPR is not straightforward. CBCT images suffer from various limitations, making direct proton dose calculations on CBCTs unreliable without corrections.This limitations include:\n",
    "\n",
    "- Low image quality, \n",
    "- Reduced field-of-view (FOV). FOV refers to the total extent of the patient anatomy that is visible within the imaging acquisition process. It represents the spatial coverage of the imaging modality, encompassing the area or volume from which data is acquired to generate the medical images\n",
    "- Inconsistencies in Hounsfield Units (HU) between conventional CBCT compared to fan-beam CT.\n",
    "\n",
    "Several correction approaches for CBCT images have been explored, including:\n",
    "\n",
    "- Look-up table (LUT) methods.\n",
    "- Histogram matching.\n",
    "- Deformation of planning CTs to match CBCT geometries, resulting in synthetic or pseudo CTs.\n",
    "- Projection-based correction methods.\n",
    "- Artificial intelligence (AI) techniques like deep convolutional neural networks and generative adversarial networks.\n",
    "\n",
    "Research comparing these methods has shown varying levels of success in improving dose calculation accuracy. For example, LUT-based methods were found less accurate than deformable image registration methods, while histogram matching and projection-based corrections have shown promise in specific anatomical regions like the head, neck, and prostate.\n",
    "\n",
    "AI techniques have introduced innovative solutions, such as using machine learning and deep learning to correct CBCT images without needing a planning CT. These methods have demonstrated high image quality and potential for accurate dose calculations, though some, like random forest-based methods and deep convolutional neural networks, showed limitations in dosimetric accuracy for certain tumor locations.\n",
    "\n",
    "The current study aims to use synthetic CT images generated from CBCT images head and neck, accounting for image quality and HU disparities, and develop a tailored registration pipeline to correct the FOV of the sCT images using that of the corresponding planification CT (pCT) to allow for online re-calculation of dosis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3f9f1",
   "metadata": {},
   "source": [
    "## **Objectives:** \n",
    "\n",
    "Develop an adaptative prontontherapy strategy to account for discrepancies between planned and delivered dose, making use of sCT images that contain CBCT image information for plan adaptation. We aim to automate the dose re-calculation process by using the anatomical information of the sCT with the HUs of the CT.\n",
    "\n",
    "For the achievement of this purpose the goal of this notebook is to apply registration theory that has been learned in the *Medical Imaging* course and gain deeper understanding of their applicability to adaptative protontherapy tasks. In particular geometric transformations, interpolation and registrations with different Degrees of Freedom (DOFs) will be implemented. Additionally deformable registrations are also implemente and relevant metrics for the evaluation of the results are calculated.\n",
    "\n",
    "We will apply these methods to a pCT and a sCT, evaluating the results and assesing how well the algorithm achieves its intended goal, searching for the geometric transformation that puts two images into spatial concordance.\n",
    "\n",
    "In addition to the self-developed registration method, a DL based approach was considered for comparison purposes. This approach employs the VoxelMorph convolutional neural network (CNN) architecture. By incorporating VoxelMorph, we aimed to evaluate the performance of a state-of-the-art deformable registration technique against our custom algorithm. This comparison helps determine the necessity and effectiveness of using a deformable registration method in our scenarios.\n",
    "\n",
    "Therefore, some necessary milestones need to be set for achieving the objetive:\n",
    "               \n",
    "   -  Finding the **spatial transformation** that makes the pCT image align with the sCT image.\n",
    "<br>\n",
    "   - **Replacing** the sCT information in the pCT image on the slices that they coincide.\n",
    "   \n",
    "   - **Training** and **testing** VoxelMorph.\n",
    "   \n",
    "   - **Evaluation** of each registration method by comparing it with a ground truth image and **comparison** between methodologies.\n",
    "\n",
    "Other **intermediate goals** include; achieving and efficient and automate registration pipeline, design and development of scripted loadable 3D Slicer module. \n",
    "\n",
    "The 3D Slicer module will include the functionality created in this registration pipeline to automate the obtention of registered images.\n",
    "\n",
    "**Methodologies:** an elastic algorithm will be implemented. The fixed image will be the pseudo CT because it contains the geometry information of the lastest patient positioning and the moving image will be the simulation CT because we are less concerned about modifying it geomertry. The CT is is used to provide the appropiate electronity density distribution map and to use for comparison of dosimetry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b5984b",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    " \n",
    " The required libraries for this project are the following ones:\n",
    " \n",
    "- **os:** It is commonly used for file and directory manipulation, such as navigating the file system and managing directories where input and output data are stored.\n",
    "\n",
    "- **itk-elastix:** The itk module, specifically itk-elastix, is used for image registration. ITK (Insight Segmentation and Registration Toolkit) provides powerful tools for medical image processing, and itk-elastix extends ITK with elastix, a widely used package for intensity-based image registration. This dependency is essential for performing the automatic 3D image registration tasks, allowing for the alignment of images through various registration algorithms provided by the elastix toolkit.\n",
    "\n",
    "- **NumPy (Numerical Python):** fundamental library for numerical computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a wide range of mathematical functions to operate on these arrays efficiently. In this project, numpy is used for array manipulation, performing mathematical operations, and handling image data.\n",
    "\n",
    "- **Nibabel:** used for reading and writing medical imaging data formats, such as NIfTI. This dependency is crucial for loading and saving medical image data, particularly in the NIfTI format, which is commonly used in neuroimaging.\n",
    "\n",
    "- **matplotlib.pyplot (Matplotlib's Pyplot Module):** this module is part of the Matplotlib library, which is used for creating static, interactive, and animated visualizations in Python. In this project, matplotlib.pyplot is utilized to visualize medical images, display intermediate results, and generate plots to help interpret the outcomes of the image registration process.\n",
    "\n",
    "- **scikit-image (Skimage):** an image processing library built on top of SciPy and NumPy. It provides tools and algorithms for a wide range of image processing tasks, making it useful for tasks such as feature extraction and image enhancement.\n",
    "\n",
    "- **SciPy:** a scientific library that builds on NumPy and provides additional functionality for scientific and technical computing. It includes tools for optimization, signal processing, statistical analysis, and more.\n",
    "\n",
    "- **TensorFlow:** TensorFlow is an open-source machine learning library. It is widely used for building and training machine learning and deep learning models. TensorFlow provides a flexible platform for constructing neural networks and other machine learning models, and it supports various deployment scenarios, including on CPUs, GPUs, and TPUs. The library offers tools and abstractions for tasks such as neural network architecture design, model training, and deployment, making it a popular choice for both researchers and practitioners in the field of machine learning.\n",
    "    - **Keras:** Keras is an open-source high-level neural networks API capable of running on top of TensorFlow, Theano, or Microsoft Cognitive Toolkit (CNTK). Keras serves as a user-friendly interface for building and experimenting with deep learning models. It simplifies the process of designing neural networks by providing a concise and expressive syntax, allowing developers to focus on model architecture and experimentation rather than low-level implementation details. Keras is often used in conjunction with TensorFlow, providing a high-level abstraction that facilitates rapid prototyping and development of deep learning models.\n",
    "    \n",
    "*TensorFlow and Keras are not directly used in the notebook but they were used for training ans testing the VoxelMorph CNN on the medical image research group (LAIMBIO) cluster computational resources*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7a8404ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itk\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.ndimage import zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90521b80",
   "metadata": {},
   "source": [
    "## Tailored Functions\n",
    "\n",
    "Here we initialize tailored functions that are used throughout the notebook.\n",
    "\n",
    "- **return_original_properties** function:\n",
    "\n",
    "This function restores the original spatial properties (spacing, origin, and direction) of a transformed image (img2) to match those of a reference image (img1). In medical imaging, images often have associated metadata that define their spatial properties:\n",
    "\n",
    "- Spacing: The physical distance between the centers of adjacent pixels/voxels.\n",
    "- Origin: The physical coordinates of the image's first pixel/voxel.\n",
    "- Direction: The orientation of the image axes.\n",
    "\n",
    "When an image is converted into an array for processing, it loses these spatial properties. This function helps restore those properties after processing is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "29b1faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spacing, origin, and direction from image\n",
    "def return_original_properties (img1, img2):\n",
    "    \"\"\"\n",
    "    Restore spatial properties of 3D image\n",
    "\n",
    "    Parameters:\n",
    "    img1: reference image to take the spatial properties from.\n",
    "    img2: image to be transformed to macth the properties of the reference image\n",
    "\n",
    "    Returns:\n",
    "    Image: image with restored origin, spacing and direction.\n",
    "    \"\"\"\n",
    "    spacing = img1.GetSpacing()\n",
    "    origin = img1.GetOrigin()\n",
    "    direction = img1.GetDirection()\n",
    "    \n",
    "    img2.SetSpacing(spacing)\n",
    "    img2.SetOrigin(origin)\n",
    "    img2.SetDirection(direction)\n",
    "    return img2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6e25b",
   "metadata": {},
   "source": [
    "- **fuse_images** function:\n",
    "\n",
    "Performs a joint display of two images, taking advantage of the correspondence between them, as they are registered.\n",
    "\n",
    "The idea is that we are going to preserve the information from the sCT where sCT and pCT coincide, since sCT represents the most recent real data of the patient with the current anatomical position, and preserve the pCT information where there is only pCT. In other words, complete the FOV of the sCT with the appropiate missing slices of the pCT.\n",
    "\n",
    "In other words, the function checks the first slice of sCT array (the numpy array representation of inverse_rigid_hu) to see if it is completely filled with the minimum value, which would indicate that it is either empty or contains background information. If this condition is met, the function iterates through the slices of sCT, replacing them with corresponding slices from pCT until it encounters non-minimum values or exceeds 18 iterations (18 was the maximum amount of empty top slices encountered in our images). This ensures that any empty or background slices at the beginning of sCT are filled with meaningful data from result_image_bspline.\n",
    "\n",
    "Similarly, the function checks the last slice of pseudo_array to see if it is completely filled with the minimum value. If true, it iterates backward through the slices, replacing them with corresponding slices from result_image_bspline until it encounters non-minimum values or exceeds 8 iterations. This step ensures that any empty or background slices at the end of pseudo_array are appropriately filled.\n",
    "\n",
    "By fusing the images in this manner, the function helps create a more comprehensive dataset that can be used for proton therapy dose recalculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ba25fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_images(result_image_bspline, inverse_rigid_hu):\n",
    "    \"\"\"\n",
    "    Fuses two 3D medical images by combining their voxel data while retaining the spatial properties of the original images.\n",
    "\n",
    "    Parameters:\n",
    "    result_image_bspline (itk.Image): The B-spline transformed image to be fused.\n",
    "    inverse_rigid_hu (itk.Image): The inverse rigid transformed image to be fused.\n",
    "\n",
    "    Returns:\n",
    "    itk.Image: synthetic CT with FOV corrected from planification CT\n",
    "    \"\"\"\n",
    "    # Convert images to numpy arrays for easier manipulation\n",
    "    ct_array = itk.GetArrayFromImage(result_image_bspline)\n",
    "    pseudo_array = itk.GetArrayFromImage(inverse_rigid_hu)\n",
    "    iterator_start = 0\n",
    "    iterator_end = 0\n",
    "    \n",
    "    # Check state of first slice and last slice\n",
    "    if np.all(pseudo_array[0, :, :] == np.min(inverse_rigid_hu[0, :, :])):\n",
    "        for i in range (0, inverse_rigid_hu.shape[0]):\n",
    "            if np.all(pseudo_array[i, :, :] == np.min(inverse_rigid_hu[0, :, :])):\n",
    "                pseudo_array[i, :, :] = result_image_bspline[i, :, :]\n",
    "            else:\n",
    "                iterator_start += 1\n",
    "                if iterator_start >= 18:\n",
    "                    break\n",
    "                else:\n",
    "                    pseudo_array[i, :, :] = result_image_bspline[i, :, :]\n",
    "                \n",
    "\n",
    "    if np.all(pseudo_array[-1, :, :] == np.min(inverse_rigid_hu[-1, :, :])):  \n",
    "        for i in range(pseudo_array.shape[0] - 1, -1, -1):\n",
    "            if np.all(pseudo_array[i, :, :] == np.min(inverse_rigid_hu[-1, :, :])):\n",
    "                pseudo_array[i, :, :] = result_image_bspline[i, :, :]\n",
    "            else:\n",
    "                iterator_end += 1\n",
    "                if iterator_end >= 8:\n",
    "                    break\n",
    "                else:\n",
    "                    pseudo_array[i, :, :] = result_image_bspline[i, :, :]\n",
    "\n",
    "    fused_image = itk.GetImageFromArray(pseudo_array)\n",
    "    fused_image = return_original_properties(inverse_rigid_hu, fused_image)\n",
    "    return fused_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948e55b",
   "metadata": {},
   "source": [
    "- **normalize_ct_image** function:\n",
    "\n",
    "Normalizes the intensity values of a 3D CT (Computed Tomography) image so that all intensity values fall within the range [0, 1]. This normalization process is crucial for standardizing image data across different scans and ensuring consistency in subsequent image analysis or processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4a2aaf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ct_image(ct_image):\n",
    "    \"\"\"\n",
    "    Normalizes the intensity values ​​of a 3D CT image to the range [0, 1].\n",
    "\n",
    "    Parameters:\n",
    "    ct_image (itk.Image): CT image to be normalized.\n",
    "\n",
    "    Returns:\n",
    "    itk.Image: CT image normalized.\n",
    "    \"\"\"\n",
    "    # Convertir la imagen a un array numpy\n",
    "    ct_array = itk.array_from_image(ct_image)\n",
    "    \n",
    "    # Obtener el valor mínimo y máximo de la imagen\n",
    "    min_intensity = ct_array.min()\n",
    "    max_intensity = ct_array.max()\n",
    "    \n",
    "    # Normalizar la imagen al rango [0, 1]\n",
    "    normalized_array = (ct_array - min_intensity) / (max_intensity - min_intensity)\n",
    "    \n",
    "    # Convertir el array numpy normalizado de vuelta a una imagen ITK\n",
    "    normalized_image = itk.image_from_array(normalized_array)\n",
    "    \n",
    "    # Copiar la información de origen de la imagen original a la normalizada\n",
    "    normalized_image = return_original_properties(ct_image, normalized_image)\n",
    "    \n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b5247",
   "metadata": {},
   "source": [
    "- **subsample_volume** function:\n",
    "\n",
    "It downsamples a 3D image to match a desired resolution or size. This is useful in various image processing and analysis tasks where images need to be standardized to a common size or resolution for comparison, analysis, or further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a3809428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_volume(image, target_size = [128, 256, 256], order=3):\n",
    "    \"\"\"\n",
    "    Subsample the given volumetric image to the target size.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: Input ITK image.\n",
    "    - target_size: Desired output size (Z, Y, X).\n",
    "    - order: The order of the spline interpolation. Default is 3 (cubic interpolation).\n",
    "    \n",
    "    Returns:\n",
    "    - subsampled_image: Subsampled ITK image.\n",
    "    \"\"\"\n",
    "    image_array = itk.GetArrayViewFromImage(image)\n",
    "    original_size = image_array.shape\n",
    "    \n",
    "    zoom_factors = [\n",
    "        target_size[0] / original_size[0],\n",
    "        target_size[1] / original_size[1],\n",
    "        target_size[2] / original_size[2]\n",
    "    ]\n",
    "    \n",
    "    subsampled_array = zoom(image_array, zoom_factors, order=order)\n",
    "    subsampled_image = itk.GetImageFromArray(subsampled_array)\n",
    "    subsampled_image = return_original_properties(image, subsampled_image)\n",
    "    \n",
    "    return subsampled_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d31237",
   "metadata": {},
   "source": [
    "Mathematical metrics functions for evaluation of the registration performance are provided below. Each metric will be individually explained in the corresponding *Metrics* section. Quantitative measures of 3D Registration are analyzed using **Structural Similarity Index(SSI)**, **Mean Square Error (MSE)** and **Mutual Informatin (MI)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c6b19448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ssim(image1, image2, data_range=1.0):\n",
    "    \"\"\"\n",
    "    Calculate the Structural Similarity Index (SSIM) for two volumetric images.\n",
    "    \n",
    "    Parameters:\n",
    "    - image1 (ndarray): First volumetric image, shape (Z, Y, X).\n",
    "    - image2 (ndarray): Second volumetric image, shape (Z, Y, X).\n",
    "    - data_range (float): Range of pixel values in the images. Default is 1.0.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Average SSIM across all slices.\n",
    "    \"\"\"\n",
    "    if image1.shape != image2.shape:\n",
    "        raise ValueError(\"Both images must have the same shape (Z, Y, X)\")\n",
    "    \n",
    "    num_slices = image1.shape[0]\n",
    "    ssim_total = 0.0\n",
    "    \n",
    "    for z in range(num_slices):\n",
    "        slice1 = image1[z]\n",
    "        slice2 = image2[z]\n",
    "        \n",
    "        # Calculate SSIM for each slice\n",
    "        ssim_slice, _ = ssim(slice1, slice2, full=True, data_range=data_range)\n",
    "        ssim_total += ssim_slice\n",
    "        \n",
    "    # Average SSIM across all slices\n",
    "    avg_ssim = ssim_total / num_slices\n",
    "    return avg_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5e92aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae(image1, image2):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Absolute Error (MAE) for two volumetric images.\n",
    "    \n",
    "    Parameters:\n",
    "    - image1 (ndarray): First volumetric image, shape (Z, Y, X).\n",
    "    - image2 (ndarray): Second volumetric image, shape (Z, Y, X).\n",
    "    \n",
    "    Returns:\n",
    "    - float: Average MAE across all voxels and slices.\n",
    "    \"\"\"\n",
    "    if image1.shape != image2.shape:\n",
    "        raise ValueError(\"Both images must have the same shape (Z, Y, X)\")\n",
    "    \n",
    "    num_slices = image1.shape[0]\n",
    "    total_mae = 0.0\n",
    "    \n",
    "    for z in range(num_slices):\n",
    "        slice1 = image1[z]\n",
    "        slice2 = image2[z]\n",
    "        \n",
    "        # Calculate MAE for each slice\n",
    "        mae_slice = np.mean(np.abs(slice1 - slice2))\n",
    "        total_mae += mae_slice\n",
    "        \n",
    "    # Average MAE across all slices and voxels\n",
    "    avg_mae = total_mae / (num_slices * np.prod(image1.shape[1:]))  # Divide by total number of voxels\n",
    "    return avg_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f89c3746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(image1, image2):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error (MSE) between two 3D images.\n",
    "\n",
    "    Parameters:\n",
    "    - image1: NumPy array representing the first 3D image.\n",
    "    - image2: NumPy array representing the second 3D image.\n",
    "\n",
    "    Returns:\n",
    "    - mse: Mean Squared Error between the two images.\n",
    "    \"\"\"\n",
    "    if image1.shape != image2.shape:\n",
    "        raise ValueError(\"Input images must have the same shape.\")\n",
    "\n",
    "    mse = np.mean((image1 - image2)**2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "25ee2356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(hgram):\n",
    "    \"\"\"\n",
    "    Calculate the mutual information for a joint histogram.\n",
    "    \n",
    "    Parameters:\n",
    "    - hgram: 2D joint histogram of the two images.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Mutual information value.\n",
    "    \"\"\"\n",
    "    # Convert bins counts to probability values\n",
    "    pxy = hgram / float(np.sum(hgram))\n",
    "    px = np.sum(pxy, axis=1)  # marginal for x over y\n",
    "    py = np.sum(pxy, axis=0)  # marginal for y over x\n",
    "    px_py = px[:, None] * py[None, :]  # Broadcast to multiply marginals\n",
    "    \n",
    "    # Now we can do the calculation using the pxy, px_py 2D arrays\n",
    "    nzs = pxy > 0  # Only non-zero pxy values contribute to the sum\n",
    "    return np.sum(pxy[nzs] * np.log(pxy[nzs] / px_py[nzs]))\n",
    "   \n",
    "# hist_2d, x_edges, y_edges = np.histogram2d(np.array(inverse_rigid_hu).ravel(),np.array(result_image_bspline).ravel(),bins=20)\n",
    "# mi_full = mutual_information(hist_2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a21374",
   "metadata": {},
   "source": [
    "# Loading and Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e8d97",
   "metadata": {},
   "source": [
    "**DATASET DESCRIPTION:** A dataset of 26 paired sCT, wCT images along with their repective pCT, from 7 patients (mean age 30.22 ± 23.04) that underwent treatment was retrospectively acquired at Centro de Protonterapia Quironsalud. The inclusion criteria involved the selection of patients with head and neck (H&N) cancer who had both CBCT and CT images available, obtained within a time span of one week. The CBCT-CT pairs were then used to traing a synthesis convolutional neural network for generation of synthetic CT with CBCT anatomical information. The CT images were acquired with a General Electric (Chigago, IL, USA) Revolution CT scanner with the following specifications: ASIR-60 iterative reconstruction, 120 kVp, pixel size 0.625mm, slice thickness 1.250mm, FOV 50 cm. Furthermore, metal artifact reduction (MAR) filters were applied when needed. Meanwhile, the CBCT images were acquired from equipment installed in the treatment room by IBA with the following specifications: 100 kVp, pixel size 0.5371mm, slice thickness 1 mm, FOV 30cm (Louvain-La-Neuve, Belgium). Data were retrospectively sourced from clinical studies conducted at the center, with ethical approval granted by the local Institutional Review Board and strict adherence to the ethical standards delineated in the World Medical Association’s Declaration of Helsinki. A GAMMEX medical phantom (Sun Nuclear, Melbourne, FL, USA) that included 16 tissue substitute inserts was utilized to calibrate the CT scanner and accurately calculate the SPR maps.\n",
    "\n",
    "Let's now load the sythetic CT images, the planification CT images and the weekly acquired CT images that we will work with throughout this notebook:\n",
    "\n",
    "**Pseudo CT:** synthetic CT image generated using Deep Learning with the anatomical information of a CBCT image and the HU map from a real CT. It contains limitations regarding the FOV and the number of slices, which are caused by the CBCT image acquisition hardware limitations.\n",
    "\n",
    "**Week CT:** CT image currently used for re-adaptation in cases where the patient presents different anatomy compared to the planning CT after CBCT asessment. Our goal is to eliminate this time-consuming step by automating it with our pseudo CT online dose calculation. We will only use this CT in the notebook as ground truth to validate the registration of the pseudo CT image with the planning CT, and in the future it will be used for comparison of proton therapy dose calculations.\n",
    "\n",
    "**Planning CT:** CT image used at the beginning of the proton therapy workflow to calculate dosis given and plan the patient's treatment. We will use this image to register with the pseudo CT and fuse its information to expand the FOV of the pseudo CT. We could also evaluate the quality of the registration using the CBCT, but since this is not registered, we compare it with the week CT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13f96076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "\n",
    "# patient 053 \n",
    "image_path_053_pair004_Week_CT = r'C:\\Users\\Javito\\TFG\\patient053\\pair004\\049_CT.nii'\n",
    "image_path_053_pair004_Plan_CT = r'C:\\Users\\Javito\\TFG\\patient053\\pair004\\053_CT.nii'\n",
    "image_path_053_pair004_pseudo_CT = r'C:\\Users\\Javito\\TFG\\patient053\\pair004\\sub-ADAPTAI-053_pair_004_CT_synthesized.nii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c554821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images with itk floats (itk.F). Necessary to employ elastix module\n",
    "pseudo = itk.imread(image_path_053_pair004_pseudo_CT, itk.F)\n",
    "ct = itk.imread(image_path_053_pair004_Plan_CT, itk.F)\n",
    "gt = itk.imread(image_path_053_pair004_Week_CT, itk.F)\n",
    "\n",
    "# Loading images with nibabel for visualization\n",
    "nifti_syn = nib.load(image_path_053_pair004_pseudo_CT).get_fdata().T\n",
    "nifti_ct =nib.load(image_path_053_pair004_Plan_CT).get_fdata().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31505f58",
   "metadata": {},
   "source": [
    "Now we print the Nifti headers to see in there is there is information about the header and slope of the equipment for the conversion to Hounsfield Units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8c6f096a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n",
      "NIfTI Image Header for synthesized CT: <class 'nibabel.nifti1.Nifti1Header'> object, endian='<'\n",
      "sizeof_hdr      : 348\n",
      "data_type       : b''\n",
      "db_name         : b''\n",
      "extents         : 0\n",
      "session_error   : 0\n",
      "regular         : b'r'\n",
      "dim_info        : 0\n",
      "dim             : [  3 512 512 128   1   1   1   1]\n",
      "intent_p1       : 0.0\n",
      "intent_p2       : 0.0\n",
      "intent_p3       : 0.0\n",
      "intent_code     : none\n",
      "datatype        : float64\n",
      "bitpix          : 64\n",
      "slice_start     : 0\n",
      "pixdim          : [-1.     0.625  0.625  1.25   1.     1.     1.     1.   ]\n",
      "vox_offset      : 0.0\n",
      "scl_slope       : nan\n",
      "scl_inter       : nan\n",
      "slice_end       : 0\n",
      "slice_code      : unknown\n",
      "xyzt_units      : 2\n",
      "cal_max         : 0.0\n",
      "cal_min         : 0.0\n",
      "slice_duration  : 0.0\n",
      "toffset         : 0.0\n",
      "glmax           : 0\n",
      "glmin           : 0\n",
      "descrip         : b''\n",
      "aux_file        : b''\n",
      "qform_code      : scanner\n",
      "sform_code      : scanner\n",
      "quatern_b       : -0.0\n",
      "quatern_c       : 1.0\n",
      "quatern_d       : 0.0\n",
      "qoffset_x       : 160.0\n",
      "qoffset_y       : -159.375\n",
      "qoffset_z       : -180.0\n",
      "srow_x          : [ -0.625  -0.      0.    160.   ]\n",
      "srow_y          : [  -0.       0.625   -0.    -159.375]\n",
      "srow_z          : [   0.      0.      1.25 -180.  ]\n",
      "intent_name     : b''\n",
      "magic           : b'n+1'\n",
      "\n",
      "\n",
      "NIfTI Image Header for Pseudo CT: <class 'nibabel.nifti1.Nifti1Header'> object, endian='<'\n",
      "sizeof_hdr      : 348\n",
      "data_type       : b''\n",
      "db_name         : b''\n",
      "extents         : 0\n",
      "session_error   : 0\n",
      "regular         : b''\n",
      "dim_info        : 0\n",
      "dim             : [  3 512 512 265   1   1   1   1]\n",
      "intent_p1       : 0.0\n",
      "intent_p2       : 0.0\n",
      "intent_p3       : 0.0\n",
      "intent_code     : none\n",
      "datatype        : int16\n",
      "bitpix          : 16\n",
      "slice_start     : 0\n",
      "pixdim          : [-1.     0.625  0.625  1.25   1.     1.     1.     1.   ]\n",
      "vox_offset      : 0.0\n",
      "scl_slope       : nan\n",
      "scl_inter       : nan\n",
      "slice_end       : 0\n",
      "slice_code      : unknown\n",
      "xyzt_units      : 2\n",
      "cal_max         : 0.0\n",
      "cal_min         : 0.0\n",
      "slice_duration  : 0.0\n",
      "toffset         : 0.0\n",
      "glmax           : 0\n",
      "glmin           : 0\n",
      "descrip         : b''\n",
      "aux_file        : b''\n",
      "qform_code      : unknown\n",
      "sform_code      : aligned\n",
      "quatern_b       : 0.0\n",
      "quatern_c       : 1.0\n",
      "quatern_d       : 0.0\n",
      "qoffset_x       : 160.0\n",
      "qoffset_y       : -159.375\n",
      "qoffset_z       : -180.0\n",
      "srow_x          : [ -0.625   0.      0.    160.   ]\n",
      "srow_y          : [  -0.       0.625    0.    -159.375]\n",
      "srow_z          : [   0.     -0.      1.25 -180.  ]\n",
      "intent_name     : b''\n",
      "magic           : b'n+1'\n"
     ]
    }
   ],
   "source": [
    "test = nib.load(image_path_053_pair004_Plan_CT)\n",
    "print(test.dataobj.slope, test.dataobj.inter)\n",
    "\n",
    "# Print the image header\n",
    "print(\"NIfTI Image Header for synthesized CT:\", nib.load(image_path_053_pair004_pseudo_CT).header)\n",
    "print(\"\\n\")\n",
    "# Print the image header\n",
    "print(\"NIfTI Image Header for Pseudo CT:\", nib.load(image_path_053_pair004_Week_CT).header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "52dc932c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of CT image: (265, 512, 512)\n",
      "Shape of synthetic CT image from CBCT: (128, 512, 512)\n",
      "Shape from ground truth Planification CT: (265, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "# Print sizes\n",
    "print('Shape of CT image:', ct.shape)\n",
    "print('Shape of synthetic CT image from CBCT:', pseudo.shape)\n",
    "print('Shape from ground truth Planification CT:', gt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c91da",
   "metadata": {},
   "source": [
    "There is a clear difference in the number of slices in the z-axis that will need to be addressed for a succesful registration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4864b28c",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "\n",
    "Before registering two images of the same modality there are several aspects to consider:\n",
    "- **Image Resolution and Voxel Size:** Ideally, the images should have similar resolutions to ensure accurate alignment.\n",
    "- **Anatomical convergence:** Verify that both images cover the same anatomical region. If the images have different coverage, the registration may not be meaningful for the entire volume. This is an issue with our images since the synthetic CT does not cover the same FOV as the CT.\n",
    "- **Pixel scale**: Check that both images are normalized or in our case both are in the same scale of Hounsfield Units for consistency.\n",
    "- **Identify anatomical landmarks**, that can be used for manual or automatic registration, this cannot be done because it would require a professional to mark them, although we can define the centroids of the heads, which has proven to be beneficial when registering the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a817b3",
   "metadata": {},
   "source": [
    "## Image Resolution and Voxel Size\n",
    "\n",
    "Here we provide a code that checks the resolution, voxel size and physical coordinates of the first voxel of the sCT and pCT image pairs to detect any inconsisty in the properties that needs to be addressed in the preprocessing phase of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2617745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT Image Properties:\n",
      "Image Shape: itkSize3 ([512, 512, 265])\n",
      "Image Spacing (Voxel Size): itkVectorD3 ([0.625, 0.625, 1.25])\n",
      "Image Origin: itkPointD3 ([-160, 159.375, -180])\n",
      "\n",
      "PseudoCT Image Properties:\n",
      "Image Shape: itkSize3 ([512, 512, 128])\n",
      "Image Spacing (Voxel Size): itkVectorD3 ([0.625, 0.625, 1.25])\n",
      "Image Origin: itkPointD3 ([-160, 159.375, -180])\n",
      "\n",
      "Ground Truth Image Properties:\n",
      "Image Shape: itkSize3 ([512, 512, 265])\n",
      "Image Spacing (Voxel Size): itkVectorD3 ([0.625, 0.625, 1.25])\n",
      "Image Origin: itkPointD3 ([-160, 159.375, -180])\n"
     ]
    }
   ],
   "source": [
    "# Check image properties for CT image\n",
    "print(\"CT Image Properties:\")\n",
    "print(\"Image Shape:\", ct.GetLargestPossibleRegion().GetSize())\n",
    "print(\"Image Spacing (Voxel Size):\", ct.GetSpacing())\n",
    "print(\"Image Origin:\", ct.GetOrigin())\n",
    "\n",
    "# Check image properties for synthetic CT image\n",
    "print(\"\\nPseudoCT Image Properties:\")\n",
    "print(\"Image Shape:\", pseudo.GetLargestPossibleRegion().GetSize())\n",
    "print(\"Image Spacing (Voxel Size):\", pseudo.GetSpacing())\n",
    "print(\"Image Origin:\", pseudo.GetOrigin())\n",
    "\n",
    "# Check image properties for Ground Truth Planification CT\n",
    "print(\"\\nGround Truth Image Properties:\")\n",
    "print(\"Image Shape:\", gt.GetLargestPossibleRegion().GetSize())\n",
    "print(\"Image Spacing (Voxel Size):\", gt.GetSpacing())\n",
    "print(\"Image Origin:\", gt.GetOrigin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1d849a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------CAUTION: Size is not in concordance.-----------\n",
      "--------Spacing is in concordance.-----------\n",
      "--------Origin is in concordance.-----------\n"
     ]
    }
   ],
   "source": [
    "# Check image properties for CT image\n",
    "ct_shape = ct.GetLargestPossibleRegion().GetSize()\n",
    "ct_spacing = ct.GetSpacing()\n",
    "ct_origin = ct.GetOrigin()\n",
    "\n",
    "# Check image properties for pseudoCT image\n",
    "pseudo_shape = pseudo.GetLargestPossibleRegion().GetSize()\n",
    "pseudo_spacing = pseudo.GetSpacing()\n",
    "pseudo_origin = pseudo.GetOrigin()\n",
    "\n",
    "# Compare properties\n",
    "def compare_properties(property_name, ct_property, pseudo_property):\n",
    "    if ct_property == pseudo_property:\n",
    "        print(f\"--------{property_name} is in concordance.-----------\")\n",
    "    else:\n",
    "        print(f\"--------CAUTION: {property_name} is not in concordance.-----------\")\n",
    "\n",
    "# Compare size\n",
    "compare_properties(\"Size\", ct_shape, pseudo_shape)\n",
    "\n",
    "# Compare spacing\n",
    "compare_properties(\"Spacing\", ct_spacing, pseudo_spacing)\n",
    "\n",
    "# Compare origin\n",
    "compare_properties(\"Origin\", ct_origin, pseudo_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353ee0c",
   "metadata": {},
   "source": [
    "- Voxel size along each axis is the same for the synthetic CBCT and CT, hence the slice thickness is the same.\n",
    "- Origin of the image in physical coordinates is the same for the synthetic CBCT and CT.\n",
    "- 3D resolution is not the same for the ct and cbct\n",
    "\n",
    "All properties coincide except for the number of slices in the axial plane, this is what originates the FOV discrepancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f90abf",
   "metadata": {},
   "source": [
    "## Transforming to Hounsfiled Units\n",
    "\n",
    "When working with CT images it is prefered to directly utilize HU values instead of normalized values, to preserve the integrity of the original CT data and ensure that the analysis reflects the true attenuation characteristics of tissues.\n",
    "\n",
    "In proton therapy, the conversion of acquired images into HU is crucial for accurately determining the Stopping Power Ratio (SPR) of various materials used in treatment planning. This transformation ensures that the density and composition of tissues are represented consistently across imaging modalities.\n",
    "\n",
    "Even though cone-beam computed tomography is extensively utilized in ART to facilitate image registration and margin adjustments but faces limitations in directly estimating SPR, necessitating additional techniques for dose recalculations in Adaptive Proton Therapy (APT) such as is the case of a sythetic CT image with the anatomical information of the CBCT but with the quality and HU instensities of the CT.\n",
    "\n",
    "Synthetic CT images were already normalized by through min-max scaling within a standard range of -1024 to 3072 HU. To work with directly with HU and revert this normalization, the inverse transformation applies the original scaling factors to restore pixel values to their initial HU ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b7e1469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3024.0\n",
      "10338.0\n",
      "0.032387286\n",
      "0.7208163\n"
     ]
    }
   ],
   "source": [
    "# Check pixel values before transformation\n",
    "print(np.min(ct)) # background HU for air in proton-therapy slicer\n",
    "print(np.max(ct))\n",
    "print(np.min(pseudo))\n",
    "print(np.max(pseudo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc33bff",
   "metadata": {},
   "source": [
    "The standard HU range is -1024 for lo density tissue, being air the lowest and 3072 for high density tissue being bone the most dense. However the window of intensity values of the pCT is much greater. This is because in the pronton-therapy slicer used in the proton-therapy center, the background (air) is set to -3024 by default and the high maximum value can appear in the presence of metallic implants.\n",
    "\n",
    "In the next code cells we will undo the denormalization of the sCT and we will correct background intensity levels to be in concordance with the pCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d86c2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the range of values of each pair of images. If range is [0, 1] transform to hounsfield units (-1024 to +3071)\n",
    "hu_min = -1024\n",
    "hu_max = 3072\n",
    "\n",
    "# Get array from ITK to perform operations\n",
    "pseudo_array = itk.GetArrayViewFromImage(pseudo)\n",
    "ct_array = itk.GetArrayViewFromImage(ct)\n",
    "\n",
    "# Applying inverse of min-max scaling (x' = (x-xmin)/(xmax-xmin))\n",
    "if np.min(ct_array) >= 0 and np.max(ct_array) <= 1:\n",
    "    # Transform ct to Hounsfield Units\n",
    "    ct_hu = round(ct_array * (hu_max-hu_min) + hu_min)\n",
    "else:\n",
    "    ct_hu = ct\n",
    "    \n",
    "if np.min(pseudo_array) >= 0 and np.max(pseudo_array) <= 1:\n",
    "    # Transform pseudo to Hounsfield Units\n",
    "    pseudo_hu = pseudo_array * (hu_max-hu_min) + hu_min\n",
    "    pseudo_hu[pseudo_hu == -801.754630] = -999\n",
    "    pseudo_hu = np.round(pseudo_hu)\n",
    "else:\n",
    "    pseudo_hu = pseudo\n",
    "    \n",
    "# Convert back into ITK format\n",
    "ct_hu_itk = itk.GetImageFromArray(ct_hu)\n",
    "pseudo_hu_itk = itk.GetImageFromArray(pseudo_hu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199be3af",
   "metadata": {},
   "source": [
    "Now we need to adjust spacing, origin and direction matrix of the pseudo_hu_itk and ct_hu_itk to the corresponding original images because they have changed after transformating it into array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d64b89c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same spacing, origin, and direction to the first image\n",
    "ct_hu_itk = return_original_properties(ct, ct_hu_itk)\n",
    "pseudo_hu_itk = return_original_properties(pseudo, pseudo_hu_itk) \n",
    "gt_itk = return_original_properties(pseudo, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c0afb441",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3024.0\n",
      "10338.0\n",
      "-999.0\n",
      "1928.0\n",
      "(265, 512, 512)\n",
      "(128, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "# Check pixel values after liner transformation\n",
    "print(np.min(ct_hu_itk))\n",
    "print(np.max(ct_hu_itk))\n",
    "print(np.min(pseudo_hu_itk))\n",
    "print(np.max(pseudo_hu_itk))\n",
    "\n",
    "print(ct_hu_itk.shape)\n",
    "print(pseudo_hu_itk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd4352a",
   "metadata": {},
   "source": [
    "# Registration Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba9e6f8",
   "metadata": {},
   "source": [
    "Medical Image Registration is a critical process in medical imaging that involves aligning two or more images into a single coordinate system. This is essential for combining information from different images (from the same or multiple modaliteis) and improving the accuracy of diagnostic and therapeutic procedures. \n",
    "\n",
    "We can run a registration pipeline with multiple stages, using the spatial transformation result from the current stage to initialize registration at the next stage. Typically, we start with a simple spatial transformation and advance to a more complex spatial transformation. This reduces computation as we increase the degrees of freedom of the registration method.\n",
    "\n",
    "The following registration framework will be proposed with tailored parameters.\n",
    "* Rigid \n",
    "* Affine\n",
    "\n",
    "Non-elastic registration methods assume that the structures within the images do not deform and only considers translations, rotations, shearing and scaling, depending on the degrees of freedom.\n",
    "* Elastix\n",
    "\n",
    "Deformable registration accounts for deformations and more complex transformations, allowing for better alignment in cases where anatomical structures may change shape or size."
   ]
  },
  {
   "attachments": {
    "registration_diagram.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "id": "7ba2f4e3",
   "metadata": {},
   "source": [
    "![registration_diagram.jpg](attachment:registration_diagram.jpg)\n",
    "\n",
    "Now the components of a general registration framework like the one in the diagram above will be explained.\n",
    "\n",
    "First the **fixed** and **moving images** are stablished. The fixed image is the reference image to which the other image (moving image) will be aligned. It remains unchanged during the registration process. The moving image is the image that needs to be transformed to align with the fixed image. This image undergoes various transformations to find the best alignment.\n",
    "\n",
    "The **interpolator** is used to estimate intensity values at non-grid positions in the moving image. Its function that estimates the values of the points from the transformed image grid such that they fall back into the grid of the original image.\n",
    "\n",
    "The **similarity metric** is a measure of how well the moving image aligns with the fixed image. It quantifies the degree of similarity between the two images. The similarity metric provides a similarity score, which is used to assess the quality of the current alignment.\n",
    "\n",
    "The **optimizer** adjusts the parameters of the transformation to maximize (or minimize) the similarity metric. It iteratively updates the transformation parameters to improve the alignment. Common optimization techniques include gradient descent, Nelder-Mead simplex method, and evolutionary algorithms.\n",
    "\n",
    "Registration is an iterative process and in each iteration of the optimization process, a transformation is applied to the initially transformed image and similarity metric is calculated between the newly transformed image and the original image.\n",
    "\n",
    "After several iterations, the optimizer converges to the transformation parameters that best align the moving image with the fixed image. This optimized transformation is applied to the moving image to achieve the final registered volume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fabbd74",
   "metadata": {},
   "source": [
    "It is important to reiterate that we want to preserve the anatomical information in the synthetic CT, so this cannot be subject to transformations that modify the contents. However, for rigid registrations, we will se that only translations which does not cause any deformation. So for this first rigid stage we will set the sCT as moving image. \n",
    "\n",
    "By doing this the sCT is being registered to the pCT which, therefore the algorithm adjusts the sCT image with 128 slices in the axial plane to the 256 slices of the pCT image acting as padding slices for increasing the FOV.\n",
    "\n",
    "So, an initial rigid registration with the CT as fixed image and the synthetic CT as moving image is applied. Then, from the resulting image we can perform further and more complex registrations to better align the images but this time with the CT as moving image, since we do not want to modify the information from the CBCT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c7a38a",
   "metadata": {},
   "source": [
    "### Rigid Registration\n",
    "\n",
    "6 DOF: translation and rotation in xyz <br>\n",
    "**Objective:** fix big translation difference, there is neglectable rotation and add padding slices in the axial plane.\n",
    "\n",
    "From previous reference it was stablished that the best parameters to rigidly registrate CT modality images are: NN interpolator and MI similarity metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9d389d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolator: ('NearestNeighborInterpolator',)\n",
      "Optimizer: ('AdaptiveStochasticGradientDescent',)\n",
      "Metric: ('AdvancedMattesMutualInformation',)\n"
     ]
    }
   ],
   "source": [
    "parameter_object_rigid = itk.ParameterObject.New()\n",
    "default_rigid_parameter_map = parameter_object_rigid.GetDefaultParameterMap('rigid')\n",
    "default_rigid_parameter_map['AutomaticTransformInitialization'] = ['true']\n",
    "default_rigid_parameter_map['Interpolator'] = ['NearestNeighborInterpolator']\n",
    "parameter_object_rigid.AddParameterMap(default_rigid_parameter_map)\n",
    "\n",
    "print('Interpolator:', default_rigid_parameter_map['Interpolator']) # Print interpolator\n",
    "print('Optimizer:',default_rigid_parameter_map['Optimizer']) # Print optimizer algorithm used\n",
    "print('Metric:',default_rigid_parameter_map['Metric']) # Print similarity metric\n",
    "\n",
    "# Call registration function\n",
    "inverse_rigid, rigid_transform_parameters = itk.elastix_registration_method(\n",
    "    ct_hu_itk, # fixed image\n",
    "    pseudo_hu_itk, # moving image\n",
    "    parameter_object=parameter_object_rigid,\n",
    "    log_to_console=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "67917cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParameterObject (0000022D45FA9C80)\n",
      "  RTTI typeinfo:   class elastix::ParameterObject\n",
      "  Reference Count: 1\n",
      "  Modified Time: 309746\n",
      "  Debug: Off\n",
      "  Object Name: \n",
      "  Observers: \n",
      "    none\n",
      "ParameterMap 0: \n",
      "  (CenterOfRotationPoint -0.3125 -0.3125 -15)\n",
      "  (CompressResultImage \"false\")\n",
      "  (ComputeZYX \"false\")\n",
      "  (DefaultPixelValue 0)\n",
      "  (Direction 1 0 0 0 -1 0 0 0 1)\n",
      "  (FinalBSplineInterpolationOrder 3)\n",
      "  (FixedImageDimension 3)\n",
      "  (FixedInternalImagePixelType \"float\")\n",
      "  (HowToCombineTransforms \"Compose\")\n",
      "  (Index 0 0 0)\n",
      "  (InitialTransformParameterFileName \"NoInitialTransform\")\n",
      "  (MovingImageDimension 3)\n",
      "  (MovingInternalImagePixelType \"float\")\n",
      "  (NumberOfParameters 6)\n",
      "  (Origin -160 159.375 -180)\n",
      "  (ResampleInterpolator \"FinalBSplineInterpolator\")\n",
      "  (Resampler \"DefaultResampler\")\n",
      "  (ResultImageFormat \"nii\")\n",
      "  (ResultImagePixelType \"float\")\n",
      "  (Size 512 512 265)\n",
      "  (Spacing 0.625 0.625 1.25)\n",
      "  (Transform \"EulerTransform\")\n",
      "  (TransformParameters 0.0190794 0.0111962 0.00193889 3.21491 0.590927 -89.1328)\n",
      "  (UseDirectionCosines \"true\")\n",
      "\n",
      "Spatial Transformation Parameters: ('3.2149133645147008', '0.5909271253293832', '-89.13284131098254')\n"
     ]
    }
   ],
   "source": [
    "# Retrieve Spatial transformation\n",
    "# print(rigid_transform_parameters)\n",
    "\n",
    "# Access the transformation parameters from the parameter map\n",
    "parameter_map = rigid_transform_parameters.GetParameterMap(0)  \n",
    "\n",
    "# Retrieve the spatial transformation parameters\n",
    "transform_parameters = parameter_map['TransformParameters']\n",
    "\n",
    "# Print or use the transformation parameters as needed\n",
    "print(\"Spatial Transformation Parameters:\", transform_parameters[3:])\n",
    "\n",
    "translation_vector = transform_parameters[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5b88f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create registration matrix\n",
    "# translation_vector = list(transform_parameters[3:])\n",
    "# translation_vector.append(1) # 4x1 vector\n",
    "# \n",
    "# # change values to float\n",
    "# translation_vector = [float(element) for element in translation_vector]\n",
    "# \n",
    "# registration_array = np.array([\n",
    "#     [1, 0, 0],\n",
    "#     [0, 1, 0],\n",
    "#     [0, 0, 1],\n",
    "#     [0, 0, 0]\n",
    "# ])\n",
    "# \n",
    "# translation_array = np.array(translation_vector)\n",
    "# \n",
    "# # Append the vector to the last column of the array\n",
    "# result_array = np.hstack((registration_array, translation_array[:, None]))\n",
    "# \n",
    "# print(result_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fe895638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transform parameters that elastix outputs can be given to transformix as input for the transformations.\n",
    "# transformix_image_rigid = itk.transformix_filter(\n",
    "#     ct_hu_itk,\n",
    "#     parameter_object=translation_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9f7db51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save image resulting image in nii format\n",
    "itk.imwrite(inverse_rigid, r'C:\\Users\\Javito\\TFG\\Output Images\\inverse_rigid.nii')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6115e18",
   "metadata": {},
   "source": [
    "### Rigid Registration Postprocessing Step\n",
    "\n",
    "As explained earlier, the sCT image, which has fewer slices on the axial plane than the pCT, requires padding to match the slice count. These additional slices are filled as a result of the rigid registration, but they are filled with zero intensity values. Therefore, a post-processing step is necessary to replace these zero values with the background intensity values from the original image, ensuring consistency and accuracy in the registered sCT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "20b5b377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum pixel value in inverse rigid image is: -1090.5607\n"
     ]
    }
   ],
   "source": [
    "# Get array from ITK to perform operations\n",
    "inverse_rigid_array = np.array(inverse_rigid)\n",
    "print('Minimum pixel value in inverse rigid image is:', np.min(inverse_rigid_array))\n",
    "\n",
    "# Assign the median to the zero elements \n",
    "inverse_rigid_array[inverse_rigid_array == 0] = np.min(inverse_rigid_array)\n",
    "    \n",
    "# Convert back into ITK format\n",
    "inverse_rigid_hu = itk.GetImageFromArray(inverse_rigid_array)\n",
    "\n",
    "inverse_rigid_hu = return_original_properties(inverse_rigid, inverse_rigid_hu)\n",
    "\n",
    "# Save image resulting image in nii format\n",
    "itk.imwrite(inverse_rigid_hu, r'C:\\Users\\Javito\\TFG\\Output Images\\inverse_rigid_hu.nii')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce5fbf",
   "metadata": {},
   "source": [
    "From now own, more complex transformations will only be performed on the pCT image to preserve the sCT information as much as possible. The moving image will be the pCT and fixed image will be the sCT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422d24e",
   "metadata": {},
   "source": [
    "### Affine Registration\n",
    "12 DOF: translation, rotation, scaling and shearing in xyz<br>\n",
    "**Objective:** further register the images. Reduce computation before elastic registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3228d489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolator: ('NearestNeighborInterpolator',)\n",
      "Optimizer: ('AdaptiveStochasticGradientDescent',)\n",
      "Metric: ('AdvancedMattesMutualInformation',)\n"
     ]
    }
   ],
   "source": [
    "parameter_object_affine = itk.ParameterObject.New()\n",
    "\n",
    "# Load and customize parameter maps\n",
    "parameter_map_affine = parameter_object_affine.GetDefaultParameterMap(\"affine\")\n",
    "parameter_map_affine[\"Metric\"] = [\"AdvancedMattesMutualInformation\"]\n",
    "parameter_map_affine['AutomaticTransformInitialization'] = ['true']\n",
    "parameter_map_affine['Interpolator'] = ['NearestNeighborInterpolator']\n",
    "parameter_object_affine.AddParameterMap(parameter_map_affine)\n",
    "\n",
    "print('Interpolator:', parameter_map_affine['Interpolator']) # Print interpolator\n",
    "print('Optimizer:',parameter_map_affine['Optimizer']) # Print optimizer algorithm used\n",
    "print('Metric:',parameter_map_affine['Metric']) # Print similarity metric\n",
    "\n",
    "# Call registration function\n",
    "affine_image, affine_transform_parameters = itk.elastix_registration_method(\n",
    "    inverse_rigid_hu, # fixed image\n",
    "    ct_hu_itk, # moving image\n",
    "    parameter_object=parameter_object_affine,\n",
    "    log_to_console=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8c49eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save image with itk\n",
    "itk.imwrite(affine_image, r'C:\\Users\\Javito\\TFG\\Output Images\\registered_affine_ct.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ef00beb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of registered affine CT image from CBCT: (265, 512, 512)\n",
      "<class 'itk.itkImagePython.itkImageF3'>\n"
     ]
    }
   ],
   "source": [
    "print('Shape of registered affine CT image from CBCT:', affine_image.shape)\n",
    "print(type(affine_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25e9ec",
   "metadata": {},
   "source": [
    "### Elastic Registration - BSpline\n",
    "\n",
    "+15 DOF: non linear registration that causes deformation on the pCT in order to fit the sCT as perfectly as possible<br>\n",
    "**Objective:** increase evaluation parameter and improve possible insufficient registration from non-deformable methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8839aa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParameterObject (0000022D45FA9E00)\n",
      "  RTTI typeinfo:   class elastix::ParameterObject\n",
      "  Reference Count: 1\n",
      "  Modified Time: 344047\n",
      "  Debug: Off\n",
      "  Object Name: \n",
      "  Observers: \n",
      "    none\n",
      "ParameterMap 0: \n",
      "  (AutomaticParameterEstimation \"true\")\n",
      "  (CheckNumberOfSamples \"true\")\n",
      "  (DefaultPixelValue 0)\n",
      "  (FinalBSplineInterpolationOrder 3)\n",
      "  (FinalGridSpacingInPhysicalUnits 16)\n",
      "  (FixedImagePyramid \"FixedSmoothingImagePyramid\")\n",
      "  (GridSpacingSchedule 1)\n",
      "  (ImageSampler \"RandomCoordinate\")\n",
      "  (Interpolator \"LinearInterpolator\")\n",
      "  (MaximumNumberOfIterations 256)\n",
      "  (MaximumNumberOfSamplingAttempts 8)\n",
      "  (Metric \"AdvancedMattesMutualInformation\" \"TransformBendingEnergyPenalty\")\n",
      "  (Metric0Weight 1)\n",
      "  (Metric1Weight 1)\n",
      "  (MovingImagePyramid \"MovingSmoothingImagePyramid\")\n",
      "  (NewSamplesEveryIteration \"true\")\n",
      "  (NumberOfResolutions 1)\n",
      "  (NumberOfResolutions  4)\n",
      "  (NumberOfSamplesForExactGradient 4096)\n",
      "  (NumberOfSpatialSamples 2048)\n",
      "  (Optimizer \"AdaptiveStochasticGradientDescent\")\n",
      "  (Registration \"MultiMetricMultiResolutionRegistration\")\n",
      "  (ResampleInterpolator \"FinalBSplineInterpolator\")\n",
      "  (Resampler \"DefaultResampler\")\n",
      "  (ResultImageFormat \"nii\")\n",
      "  (Transform \"BSplineTransform\")\n",
      "  (WriteIterationInfo \"false\")\n",
      "  (WriteResultImage \"true\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create test images\n",
    "fixed_image_bspline = inverse_rigid_hu\n",
    "moving_image_bspline = affine_image\n",
    "\n",
    "# Import Default Parameter Map\n",
    "parameter_object = itk.ParameterObject.New()\n",
    "default_bspline_parameter_map = parameter_object.GetDefaultParameterMap('bspline',1)\n",
    "default_bspline_parameter_map['FinalBSplineInterpolationOrder'] = ['3']\n",
    "default_bspline_parameter_map['NumberOfResolutions '] = ['4']\n",
    "default_bspline_parameter_map['FinalGridSpacingInPhysicalUnits'] = ['16']\n",
    "# default_bspline_parameter_map['FinalGridSpacingInVoxels'] = ['16']\n",
    "parameter_object.AddParameterMap(default_bspline_parameter_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d712f48",
   "metadata": {},
   "source": [
    "The control point spacing of the bspline transformation in the finest resolution level. Can be specified for each dimension differently. Unit: mm. The lower this value, the more flexible the deformation. Low values may improve the accuracy, but may also cause unrealistic deformations.\n",
    "Tune it for every specific application. **(FinalGridSpacingInPhysicalUnits 16)**\n",
    "\n",
    "Alternatively, the grid spacing can be specified in voxel units.\n",
    "**(FinalGridSpacingInVoxels 16)**\n",
    "\n",
    "Order of B-Spline interpolation used during registration/optimisation.It may improve accuracy if you set this to 3. Never use 0.\n",
    "An order of 1 gives linear interpolation. This is in most applications a good choice.\n",
    "**(BSplineInterpolationOrder 1)**\n",
    "\n",
    "The number of resolutions. 1 Is only enough if the expected deformations are small. 3 or 4 mostly works fine. For large images and large deformations, 5 or 6 may even be useful.\n",
    "**(NumberOfResolutions 4)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f83497bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call registration function\n",
    "result_image_bspline, result_transform_parameters = itk.elastix_registration_method(\n",
    "    fixed_image_bspline, # fixed image\n",
    "    moving_image_bspline, # moving image\n",
    "    parameter_object=parameter_object,\n",
    "    log_to_console=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a36db433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save image with itk\n",
    "itk.imwrite(result_image_bspline, r'C:\\Users\\Javito\\TFG\\Output Images\\bspline_registration.nii')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b8d237",
   "metadata": {},
   "source": [
    "## Evaluating Registration\n",
    "\n",
    "The quality of the 3D registration algorithm is analyzed using mathematical parameters that help determine an objective metric for the similarity between two images:\n",
    "\n",
    "The efficiency of the algorithm was quantified in terms of **Mean Square Error (MSE)**, **Mean Absolute Error (MAE)**, **Structural Similarity Index (SSI)** and **Mutual Information (MI):**\n",
    "\n",
    "- **Mean Square Error (MSE):** MSE indicates the geometric alignment of the two 3D images. MSE=0, indicates the perfect alignment and two images whereas MSE>0 indicates Misalignment. The HU value pair at index i, j, k is compared.\n",
    "\n",
    "- **Mean Absolute Error (MAE):** measures the average magnitude of errors in registration between two 3D images. It is computed as the average absolute difference between corresponding Hounsfield Unit (HU) values at each voxel across the entire image volume. MAE is used in medical image analysis for registration evaluation because it treats all registration errors equally, providing a balanced assessment of alignment quality across the entire image volume. This approach ensures that outliers or localized misalignments do not disproportionately affect the evaluation. The HU value pair at index i, j, k is compared.\n",
    "\n",
    "- **Mutual information metric:** the reason a 2D histogram is used is because you we computing the occurrence of pixel amplitudes in two images. Basically, in this implementation, each ND image is reduced to a 1D distribution, and the 1D distributions are used to build a joint (2D) histogram.\n",
    "\n",
    "    This function will be implemented to calculate the mutual information between the two images. Mutual Information is an intensity-based measure that does not rely on specific image features and is used as a similarity metric to quantify the amount of information shared between two images, measuring the reduction in uncertainty about one image when the other image is known. The optimization process aims to find the appropiate transformation parameters that minimize this measure, which would indicate a good alignment between the images.\n",
    "\n",
    "    <center> MI(I,J)=H(I)+H(J)−H(I,J) </center>\n",
    "\n",
    "    H(X), H(Y) represents the entropy of X and Y 3D image dataset respectively, whereas H(X, Y) represents the joint entropy of both the 3D image datasets.\n",
    "\n",
    "- **Structure Similarity Index (SSI):** provides a comprehensive measure of similarity that considers not only the intensity distribution (luminance) but also the variations in intensity (contrast) and the relationship between neighboring pixel values (structure function) across the entire 3D image datasets.\n",
    "\n",
    "In this section we will analyze the two 3D registration algorithms, one consists of a **non-deformable registration** method (Rigid + Affine) and the other includes an additional **deformable registration** stage (Rigid + Affine + Elastic).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b8b64d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of mean score results for each metric to latter create a dataframe\n",
    "\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "ssi_scores = []\n",
    "mi_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c9596d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of access paths to fused affine images\n",
    "affine_file_names = [\n",
    "    \"fused_affine_053_pair004\",\n",
    "    \"fused_affine_054_pair000\",\n",
    "    \"fused_affine_054_pair001\",\n",
    "    \"fused_affine_054_pair002\",\n",
    "    \"fused_affine_054_pair003\",\n",
    "    \"fused_affine_054_pair004\",\n",
    "    \"fused_affine_055_pair000\",\n",
    "    \"fused_affine_055_pair001\",\n",
    "    \"fused_affine_055_pair002\",\n",
    "    \"fused_affine_055_pair004\",\n",
    "    \"fused_affine_055_pair005\",\n",
    "    \"fused_affine_055_pair006\",\n",
    "    \"fused_affine_056_pair000\",\n",
    "    \"fused_affine_056_pair001\",\n",
    "    \"fused_affine_057_pair000\",\n",
    "    \"fused_affine_057_pair001\",\n",
    "    \"fused_affine_057_pair002\",\n",
    "    \"fused_affine_057_pair003\",\n",
    "    \"fused_affine_058_pair000\",\n",
    "    \"fused_affine_058_pair001\",\n",
    "    \"fused_affine_058_pair002\",\n",
    "    \"fused_affine_058_pair003\",\n",
    "    \"fused_affine_058_pair004\",\n",
    "    \"fused_affine_059_pair000\",\n",
    "    \"fused_affine_059_pair001\"\n",
    "]\n",
    "\n",
    "# Base path\n",
    "base_path_affine = r\"Z:\\MATERIA_OSCURA\\TFGs\\2020_JavierVilloldo\\002-DATA\\Fused_Affine\"\n",
    "\n",
    "# Adding the base path to each file name\n",
    "affine_paths_list = [f\"{base_path_affine}\\\\{affine_file_name}\" for affine_file_name in affine_file_names]\n",
    "\n",
    "\n",
    "# Create list of access paths to ground truth images (wCT)\n",
    "gt_file_names = [\n",
    "    \"053_004_CT\",\n",
    "    \"054_000_CT\",\n",
    "    \"054_001_CT\",\n",
    "    \"054_002_CT\",\n",
    "    \"054_003_CT\",\n",
    "    \"054_004_CT\",\n",
    "    \"055_000_CT\",\n",
    "    \"055_001_CT\",\n",
    "    \"055_002_CT\",\n",
    "    \"055_004_CT\",\n",
    "    \"055_005_CT\",\n",
    "    \"055_006_CT\",\n",
    "    \"056_000_CT\",\n",
    "    \"056_001_CT\",\n",
    "    \"057_000_CT\",\n",
    "    \"057_001_CT\",\n",
    "    \"057_002_CT\",\n",
    "    \"057_003_CT\",\n",
    "    \"058_000_CT\",\n",
    "    \"058_001_CT\",\n",
    "    \"058_002_CT\",\n",
    "    \"058_003_CT\",\n",
    "    \"058_004_CT\",\n",
    "    \"059_000_CT\",\n",
    "    \"059_001_CT\"\n",
    "]\n",
    "\n",
    "# Base path\n",
    "base_path_gt = r\"Z:\\MATERIA_OSCURA\\TFGs\\2020_JavierVilloldo\\002-DATA\\weekCT\"\n",
    "\n",
    "# Adding the base path to each file name\n",
    "gt_paths_list = [f\"{base_path_gt}\\\\{gt_file_name}\" for gt_file_name in gt_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c77535be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0069589852 2.472341417822282e-07 0.7275117981003741 0.5328181120766871\n"
     ]
    }
   ],
   "source": [
    "mse_affine = []\n",
    "mae_affine = []\n",
    "ssi_affine = []\n",
    "mi_affine = []\n",
    "\n",
    "for i in range (0, len(gt_file_names)):\n",
    "    # Load images\n",
    "    affine_result = itk.imread(affine_paths_list[i], itk.F)\n",
    "    gt = itk.imread(gt_paths_list[i], itk.F)\n",
    "    \n",
    "    # Normalize\n",
    "    norm_affine = normalize_ct_image(affine_result)\n",
    "    norm_gt = normalize_ct_image(gt)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    # Convert ITK images to numpy arrays\n",
    "    norm_affine_np = itk.array_from_image(norm_affine)\n",
    "    norm_gt_np = itk.array_from_image(norm_gt)\n",
    "\n",
    "    mse = calculate_mse(norm_affine_np, norm_gt_np)\n",
    "    mae = calculate_mae(norm_affine_np, norm_gt_np)\n",
    "    ssim_val = calculate_ssim(norm_affine_np, norm_gt_np)\n",
    "    hist_2d, x_edges, y_edges = np.histogram2d(norm_affine_np.ravel(),norm_gt_np.ravel(),bins=20)\n",
    "    mi = mutual_information(hist_2d)\n",
    "    \n",
    "    # Append values to the lists\n",
    "    mse_affine.append(mse)\n",
    "    mae_affine.append(mae)\n",
    "    ssi_affine.append(ssim_val)\n",
    "    mi_affine.append(mi)\n",
    "    \n",
    "# Calculate average results\n",
    "mse_affine_mean = np.mean(mse_affine)\n",
    "mae_affine_mean = np.mean(mae_affine)\n",
    "ssi_affine_mean = np.mean(ssi_affine)\n",
    "mi_affine_mean = np.mean(mi_affine)\n",
    "\n",
    "print(mse_affine_mean, mae_affine_mean, ssi_affine_mean, mi_affine_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1a063e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mean results for dataframe\n",
    "mse_scores.append(mse_affine_mean)\n",
    "mae_scores.append(mae_affine_mean)\n",
    "ssi_scores.append(ssi_affine_mean)\n",
    "mi_scores.append(mi_affine_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "65efd0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of access paths to fused elastix register images results\n",
    "elastix_file_names = [\n",
    "    \"result_fused_elastic_053_004\",\n",
    "    \"result_fused_elastic_054_000\",\n",
    "    \"result_fused_elastic_054_001\",\n",
    "    \"result_fused_elastic_054_002\",\n",
    "    \"result_fused_elastic_054_003\",\n",
    "    \"result_fused_elastic_054_004\",\n",
    "    \"result_fused_elastic_055_000\",\n",
    "    \"result_fused_elastic_055_001\",\n",
    "    \"result_fused_elastic_055_002\",\n",
    "    \"result_fused_elastic_055_004\",\n",
    "    \"result_fused_elastic_055_005\",\n",
    "    \"result_fused_elastic_055_006\",\n",
    "    \"result_fused_elastic_056_000\",\n",
    "    \"result_fused_elastic_056_001\",\n",
    "    \"result_fused_elastic_057_000\",\n",
    "    \"result_fused_elastic_057_001\",\n",
    "    \"result_fused_elastic_057_002\",\n",
    "    \"result_fused_elastic_057_003\",\n",
    "    \"result_fused_elastic_058_000\",\n",
    "    \"result_fused_elastic_058_001\",\n",
    "    \"result_fused_elastic_058_002\",\n",
    "    \"result_fused_elastic_058_003\",\n",
    "    \"result_fused_elastic_058_004\",\n",
    "    \"result_fused_elastic_059_000\",\n",
    "    \"result_fused_elastic_059_001\"\n",
    "]\n",
    "\n",
    "# Base path\n",
    "base_path_elastix = r\"Z:\\MATERIA_OSCURA\\TFGs\\2020_JavierVilloldo\\002-DATA\\Fused_Elastix_Register\"\n",
    "\n",
    "# Adding the base path to each file name\n",
    "elastix_paths_list = [f\"{base_path_elastix}\\\\{elastix_file_name}\" for elastix_file_name in elastix_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "791a64a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009311977 3.0985579702297566e-07 0.7104892448204093 0.5312188560811574\n"
     ]
    }
   ],
   "source": [
    "mse_elastix = []\n",
    "mae_elastix = []\n",
    "ssi_elastix = []\n",
    "mi_elastix = []\n",
    "\n",
    "for i in range (0, len(gt_file_names)):\n",
    "    # Load images\n",
    "    elatix_result = itk.imread(elastix_paths_list[i], itk.F)\n",
    "    gt = itk.imread(gt_paths_list[i], itk.F)\n",
    "    \n",
    "    # Normalize\n",
    "    norm_elastix = normalize_ct_image(elatix_result)\n",
    "    norm_gt = normalize_ct_image(gt)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    # Convert ITK images to numpy arrays\n",
    "    norm_elastix_np = itk.array_from_image(norm_elastix)\n",
    "    norm_gt_np = itk.array_from_image(norm_gt)\n",
    "\n",
    "    mse = calculate_mse(norm_elastix_np, norm_gt_np)\n",
    "    mae = calculate_mae(norm_elastix_np, norm_gt_np)\n",
    "    ssim_val = calculate_ssim(norm_elastix_np, norm_gt_np)\n",
    "    hist_2d, x_edges, y_edges = np.histogram2d(norm_elastix_np.ravel(),norm_gt_np.ravel(),bins=20)\n",
    "    mi = mutual_information(hist_2d)\n",
    "    \n",
    "    # Append values to the lists\n",
    "    mse_elastix.append(mse)\n",
    "    mae_elastix.append(mae)\n",
    "    ssi_elastix.append(ssim_val)\n",
    "    mi_elastix.append(mi)\n",
    "    \n",
    "# Calculate average results\n",
    "mse_elastix_mean = np.mean(mse_elastix)\n",
    "mae_elastix_mean = np.mean(mae_elastix)\n",
    "ssi_elastix_mean = np.mean(ssi_elastix)\n",
    "mi_elastix_mean = np.mean(mi_elastix)\n",
    "\n",
    "print(mse_elastix_mean, mae_elastix_mean, ssi_elastix_mean, mi_elastix_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "aabe0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mean results for dataframe\n",
    "mse_scores.append(mse_elastix_mean)\n",
    "mae_scores.append(mae_elastix_mean)\n",
    "ssi_scores.append(ssi_elastix_mean)\n",
    "mi_scores.append(mi_elastix_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a345e9e",
   "metadata": {},
   "source": [
    "# Image Fusion\n",
    "\n",
    "As it has already been explained in the *Tailored Functions* section, with the fusion of image information we want to preserve the information from the sCT where sCT and pCT coincide, since the sCT represents the most recent real data of the patient's anatomy, and preserve the pCT information where there are backgrtound slices in the sCT.\n",
    "\n",
    "For clarification purposes, new names for the images have been adopted after registration:\n",
    "- The sCT is now refered to as **inverse rigid** because it was set as the moving image in an initial rigid registration.\n",
    "- The pCT is now reference as **register affine image** because it was set as the moving image in the affine registration for further aligning the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "192cc9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_image = fuse_images(affine_image, inverse_rigid_hu)\n",
    "itk.imwrite(fused_image, r'C:\\Users\\Javito\\TFG\\Output Images\\result_affine_fused_054_001.nii')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162aa4ee",
   "metadata": {},
   "source": [
    "# Deep-Learning Method - VoxelMorph Learning-based Image Registration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0ea7e",
   "metadata": {},
   "source": [
    "## VoxelMorph Preprocessing\n",
    "\n",
    "In this section, we will prepare our input images for the registration Convolutional Neural Network (CNN) by ensuring they meet the format requirements of the VoxelMorph framework. This involves two main preprocessing steps. Normalization of intensity values of the images. Adjust the resolution of the images by subsampling them to dimensions that are powers of two. This adjustment not only conforms to the network's requirements but also helps manage memory efficiently during the computation. These preprocessing steps are crucial to ensure tha t the input data is properly formatted and optimized for accurate and efficient registration using the VoxelMorph CNN.\n",
    "\n",
    "VoxelMorph is and unsupervised learning methods so we only need to define input images for training and testing, we have chosen the following:\n",
    "- **Fixed Image:** this will be the result from the rigid registration, the **inverse_rigid** image, which corresponds to the sCT.\n",
    "- **Moving Image:** this will be the pCT, since this CNN implements elastic deformation we want to register the pCT with respect to the sCT in order to preserve sCT information about the patient's anatomy.\n",
    "\n",
    "***Note:** A detailed explation of the VoxelMorph architecture, training, testing, the data split, training parameters and loss functions used is provided in the memory of the thesis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8f532809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (321, 512, 512)\n",
      "Subsampled shape: (128, 256, 256)\n",
      "(128, 256, 256)\n",
      "<class 'itk.itkImagePython.itkImageF3'>\n"
     ]
    }
   ],
   "source": [
    "fused_affine_arr = itk.GetArrayViewFromImage(fused_image)\n",
    "print('Original shape:', fused_affine_arr.shape)\n",
    "# Define the desired subsampled size\n",
    "subsampled_size = [128, 256, 256]\n",
    "\n",
    "# Calculate the zoom factors\n",
    "zoom_factors = [\n",
    "    subsampled_size[0] / fused_affine_arr.shape[0],\n",
    "    subsampled_size[1] / fused_affine_arr.shape[1],\n",
    "    subsampled_size[2] / fused_affine_arr.shape[2]\n",
    "]\n",
    "\n",
    "# Perform the subsampling\n",
    "subsampled_fused_affine_arr = zoom(fused_affine_arr, zoom_factors, order=3)  # order=3 for cubic interpolation\n",
    "print('Subsampled shape:', subsampled_fused_affine_arr.shape)\n",
    "\n",
    "# Convert back into ITK format\n",
    "subsampled_fused_affine = itk.GetImageFromArray(subsampled_fused_affine_arr)\n",
    "subs_fused_affine = return_original_properties(affine_image, subsampled_fused_affine)\n",
    "\n",
    "print(subs_fused_affine.shape)\n",
    "print(type(subs_fused_affine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b8f059f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "itk.imwrite(subs_fused_affine, r'C:\\Users\\Javito\\TFG\\Output Images\\subsampled_result_fused_affine_054_001.nii')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8415136",
   "metadata": {},
   "source": [
    "## VoxelMorph Results Fusion and Evaluation\n",
    "\n",
    "The resulting image from this DL approach will then be fused to add the additional FOV information from the pCT to the sCT and this image will, in turn, be compared to the grounth truth image (week CT) for evaluation of the registration capabilities of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d15f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create List of access paths to results of the VoxelMorph\n",
    "vm_file_names = [\n",
    "    \"053_pair_004\",\n",
    "    \"054_pair_000\",\n",
    "    \"054_pair_001\",\n",
    "    \"054_pair_002\",\n",
    "    \"054_pair_003\",\n",
    "    \"054_pair_004\",\n",
    "    \"055_pair_000\",\n",
    "    \"055_pair_001\",\n",
    "    \"055_pair_002\",\n",
    "    \"055_pair_003\",\n",
    "    \"055_pair_004\",\n",
    "    \"055_pair_005\",\n",
    "    \"055_pair_006\",\n",
    "    \"056_pair_000\",\n",
    "    \"056_pair_001\",\n",
    "    \"057_pair_000\",\n",
    "    \"057_pair_001\",\n",
    "    \"057_pair_002\",\n",
    "    \"057_pair_003\",\n",
    "    \"058_pair_000\",\n",
    "    \"058_pair_001\",\n",
    "    \"058_pair_002\",\n",
    "    \"058_pair_003\",\n",
    "    \"058_pair_004\",\n",
    "    \"059_pair_000\",\n",
    "    \"059_pair_001\"\n",
    "]\n",
    "\n",
    "# Base path\n",
    "base_path_vm_results = r\"Z:\\MATERIA_OSCURA\\TFGs\\2020_JavierVilloldo\\002-DATA\\result_Voxelmorph\"\n",
    "\n",
    "# Adding the base path to each folder name\n",
    "vm_results_path_list = [f\"{base_path_vm_results}\\\\{vm_file_name}\" for vm_file_name in vm_file_names]\n",
    "\n",
    "# Create list of access paths to sCT with padding slices (FOV correction)\n",
    "pseudo_file_names = [\n",
    "    \"rigid_fov_053_pair004\",\n",
    "    \"rigid_fov_054_pair000\",\n",
    "    \"rigid_fov_054_pair001\",\n",
    "    \"rigid_fov_054_pair002\",\n",
    "    \"rigid_fov_054_pair003\",\n",
    "    \"rigid_fov_054_pair004\",\n",
    "    \"rigid_fov_055_pair000\",\n",
    "    \"rigid_fov_055_pair001\",\n",
    "    \"rigid_fov_055_pair002\",\n",
    "    \"rigid_fov_055_pair003\",\n",
    "    \"rigid_fov_055_pair004\",\n",
    "    \"rigid_fov_055_pair005\",\n",
    "    \"rigid_fov_055_pair006\",\n",
    "    \"rigid_fov_056_pair000\",\n",
    "    \"rigid_fov_056_pair001\",\n",
    "    \"rigid_fov_057_pair000\",\n",
    "    \"rigid_fov_057_pair001\",\n",
    "    \"rigid_fov_057_pair002\",\n",
    "    \"rigid_fov_057_pair003\",\n",
    "    \"rigid_fov_058_pair000\",\n",
    "    \"rigid_fov_058_pair001\",\n",
    "    \"rigid_fov_058_pair002\",\n",
    "    \"rigid_fov_058_pair003\",\n",
    "    \"rigid_fov_058_pair004\",\n",
    "    \"rigid_fov_059_pair000\",\n",
    "    \"rigid_fov_059_pair001\"\n",
    "]\n",
    "\n",
    "# Base path\n",
    "base_path_pseudo = r\"Z:\\MATERIA_OSCURA\\TFGs\\2020_JavierVilloldo\\002-DATA\\VoxelMorph\\rigid_synthetic\"\n",
    "\n",
    "# Adding the base path to each file name\n",
    "rigid_pseudo_paths_list = [f\"{base_path_pseudo}\\\\{pseudo_file_name}\" for pseudo_file_name in pseudo_file_names]\n",
    "\n",
    "# Create list of access paths to ground truth images (wCT)\n",
    "gt_file_names = [\n",
    "    \"053_004_CT\",\n",
    "    \"054_000_CT\",\n",
    "    \"054_001_CT\",\n",
    "    \"054_002_CT\",\n",
    "    \"054_003_CT\",\n",
    "    \"054_004_CT\",\n",
    "    \"055_000_CT\",\n",
    "    \"055_001_CT\",\n",
    "    \"055_002_CT\",\n",
    "    \"055_003_CT\",\n",
    "    \"055_004_CT\",\n",
    "    \"055_005_CT\",\n",
    "    \"055_006_CT\",\n",
    "    \"056_000_CT\",\n",
    "    \"056_001_CT\",\n",
    "    \"057_000_CT\",\n",
    "    \"057_001_CT\",\n",
    "    \"057_002_CT\",\n",
    "    \"057_003_CT\",\n",
    "    \"058_000_CT\",\n",
    "    \"058_001_CT\",\n",
    "    \"058_002_CT\",\n",
    "    \"058_003_CT\",\n",
    "    \"058_004_CT\",\n",
    "    \"059_000_CT\",\n",
    "    \"059_001_CT\"\n",
    "]\n",
    "\n",
    "# Base path\n",
    "base_path_gt = r\"Z:\\MATERIA_OSCURA\\TFGs\\2020_JavierVilloldo\\002-DATA\\weekCT\"\n",
    "\n",
    "# Adding the base path to each file name\n",
    "gt_paths_list = [f\"{base_path_gt}\\\\{gt_file_name}\" for gt_file_name in gt_file_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a57d9",
   "metadata": {},
   "source": [
    "To avoid having to save and then load lots of imagenes generated during the next steps. We will create a for loop that performs the following tasks:\n",
    "\n",
    "- Fuse FOV from VoxelMorph results to the sCT (the one obtained after registration).\n",
    "- Normalize and subsample GT images, to bring them to a common resolution and scale for comparison with the fused images.\n",
    "- Calculate similarity metrics between fused images and GT.\n",
    "- Create a dataframe that saves the metric's measurements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a3875d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020100137 1.8782298718544865e-06 0.5094542358049374 0.17100710683699455\n"
     ]
    }
   ],
   "source": [
    "mse_cnn = []\n",
    "mae_cnn = []\n",
    "ssi_cnn = []\n",
    "mi_cnn = []\n",
    "\n",
    "for i in range (0, len(gt_file_names)):\n",
    "    # Load images\n",
    "    vm_result = itk.imread(vm_results_path_list[i], itk.F)\n",
    "    rigid_pseudo = itk.imread(rigid_pseudo_paths_list[i], itk.F)\n",
    "    gt = itk.imread(gt_paths_list[i], itk.F)\n",
    "    \n",
    "    # Fuse images\n",
    "    final_result = fuse_images(vm_result, rigid_pseudo)\n",
    "    \n",
    "    # Normalize and subsample\n",
    "    gt_subsampled = subsample_volume(gt)\n",
    "    norm_gt_subsampled = normalize_ct_image(gt_subsampled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    # Convert ITK images to numpy arrays\n",
    "    final_result_np = itk.array_from_image(final_result)\n",
    "    norm_gt_subsampled_np = itk.array_from_image(norm_gt_subsampled)\n",
    "\n",
    "    mse = calculate_mse(final_result_np, norm_gt_subsampled_np)\n",
    "    mae = calculate_mae(final_result_np, norm_gt_subsampled_np)\n",
    "    ssim_val = calculate_ssim(final_result_np, norm_gt_subsampled_np)\n",
    "    hist_2d, x_edges, y_edges = np.histogram2d(final_result_np.ravel(),norm_gt_subsampled_np.ravel(),bins=20)\n",
    "    mi = mutual_information(hist_2d)\n",
    "    \n",
    "    # Append values to the lists\n",
    "    mse_cnn.append(mse)\n",
    "    mae_cnn.append(mae)\n",
    "    ssi_cnn.append(ssim_val)\n",
    "    mi_cnn.append(mi)\n",
    "    \n",
    "# Calculate average results\n",
    "mse_cnn_mean = np.mean(mse_cnn)\n",
    "mae_cnn_mean = np.mean(mae_cnn)\n",
    "ssi_cnn_mean = np.mean(ssi_cnn)\n",
    "mi_cnn_mean = np.mean(mi_cnn)\n",
    "\n",
    "print(mse_cnn_mean, mae_cnn_mean, ssi_cnn_mean, mi_cnn_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bee0ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mean results for dataframe\n",
    "mse_scores.append(mse_cnn_mean)\n",
    "mae_scores.append(mae_cnn_mean)\n",
    "ssi_scores.append(ssi_cnn_mean)\n",
    "mi_scores.append(mi_cnn_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f177f1a",
   "metadata": {},
   "source": [
    "# Statistical Analysis of Methods\n",
    "\n",
    "The ANOVA test, or Analysis of Variance test, is a statistical technique used to compare means between two or more groups. It assesses whether there are any statistically significant differences between the means of these groups. In our case, ANOVA is used to compare the results obtained from the different registration methods.\n",
    "\n",
    "This method allows to go beyond simple descriptive comparisons and visual inspections and make statistically sound conclusions about which method, if any, produces outcomes that are significantly different from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "62aba0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>SSI</th>\n",
       "      <th>MI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Methods</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Non-Deformable</th>\n",
       "      <td>0.006959</td>\n",
       "      <td>2.472341e-07</td>\n",
       "      <td>0.727512</td>\n",
       "      <td>0.532818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elastix</th>\n",
       "      <td>0.009312</td>\n",
       "      <td>3.098558e-07</td>\n",
       "      <td>0.710489</td>\n",
       "      <td>0.531219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VoxelMorph</th>\n",
       "      <td>0.020100</td>\n",
       "      <td>1.878230e-06</td>\n",
       "      <td>0.509454</td>\n",
       "      <td>0.171007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     MSE           MAE       SSI        MI\n",
       "Methods                                                   \n",
       "Non-Deformable  0.006959  2.472341e-07  0.727512  0.532818\n",
       "Elastix         0.009312  3.098558e-07  0.710489  0.531219\n",
       "VoxelMorph      0.020100  1.878230e-06  0.509454  0.171007"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "results[\"MSE\"] = mse_scores\n",
    "results[\"MAE\"] = mae_scores\n",
    "results[\"SSI\"] = ssi_scores\n",
    "results[\"MI\"] = mi_scores\n",
    "\n",
    "#results\n",
    "results[\"Methods\"] = [\"Non-Deformable\", \"Elastix\", \"VoxelMorph\"]\n",
    "results.set_index(\"Methods\", inplace = True)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57264abf",
   "metadata": {},
   "source": [
    "These results are not direct evaluation of the registered images. \n",
    "\n",
    "Lets clarify, in image registration, where there is a fixed and moving image the goal is to find the transformation applied to the moving image that best puts the moving image into spatial concordance with the fixed one. Evaluating the quality of a registration algorithm involves assessing how well the moving image has been aligned with the fixed image. There are several metrics and methods that can be used to evaluate the performance of registration algorithms. They are mostly based on intensity-based metrics (MAE, MSE), structural metrics (SSI) or information theory-based metrics (MI) that compare the resgistration result with the fixed image.\n",
    "\n",
    "With the provided evalution the fixed and moving images are not being compared directly. Instead a correction of FOV is done after each registration result and its that fused image image (registration result with FOV correction) the one compared with the week CT, that has been considered as the Ground Truth, since its the one used for dose recalculation. This is somes implies comparisons of images that are not registered since the GT has not been modified. Instead of focusing on the absolute value of the metric we have to look at them with respect to the results in the rest of the registration process\n",
    "\n",
    "Based on the results provided above, two conclusions can be drawned. First, the Non-Deformable method seems performs best across all metrics since it has the lowest MSE and MAE and the highest MI and SSI which indicates that the image obtained with the Rigid and Affine registration is the most similar to the GT. Secondly, there is no need for deformable registration methods. Deformable methods, with their higher degrees of freedom, may overfit to local variations in the images. This can lead to distortions and misalignments that degrade performance metrics like MSE, MAE, SSI, and MI.\n",
    "\n",
    "Now an **ANOVA test** will be performed to compare statistical significance between the non-deformable method and the methods that implement elastic deformation (elastix and VoxelMorph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "82d7c0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Methods</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>SSI</th>\n",
       "      <th>MI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Non-Deformable</td>\n",
       "      <td>0.006959</td>\n",
       "      <td>2.472341e-07</td>\n",
       "      <td>0.727512</td>\n",
       "      <td>0.532818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elastic Methods</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>1.094043e-06</td>\n",
       "      <td>0.609972</td>\n",
       "      <td>0.351113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Methods       MSE           MAE       SSI        MI\n",
       "0   Non-Deformable  0.006959  2.472341e-07  0.727512  0.532818\n",
       "1  Elastic Methods  0.014706  1.094043e-06  0.609972  0.351113"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA results for MSE:\n",
      "F-statistic: 0.6875734683813701, p-value: 0.5592717832245263\n",
      "\n",
      "ANOVA results for MAE:\n",
      "F-statistic: 0.3886959736381619, p-value: 0.6450924739913554\n",
      "\n",
      "ANOVA results for SSI:\n",
      "F-statistic: 0.4557924730170173, p-value: 0.6219529751999575\n",
      "\n",
      "ANOVA results for MI:\n",
      "F-statistic: 0.33927930380512905, p-value: 0.6642243289139866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "\n",
    "# Combine \"Elastix\" and \"VoxelMorph\" into a single method\n",
    "combined_results = pd.DataFrame({\n",
    "    \"Methods\": [\"Non-Deformable\", \"Elastic Methods\"],\n",
    "    \"MSE\": [results.loc[\"Non-Deformable\", \"MSE\"], results.loc[[\"Elastix\", \"VoxelMorph\"], \"MSE\"].mean()],\n",
    "    \"MAE\": [results.loc[\"Non-Deformable\", \"MAE\"], results.loc[[\"Elastix\", \"VoxelMorph\"], \"MAE\"].mean()],\n",
    "    \"SSI\": [results.loc[\"Non-Deformable\", \"SSI\"], results.loc[[\"Elastix\", \"VoxelMorph\"], \"SSI\"].mean()],\n",
    "    \"MI\": [results.loc[\"Non-Deformable\", \"MI\"], results.loc[[\"Elastix\", \"VoxelMorph\"], \"MI\"].mean()]\n",
    "})\n",
    "\n",
    "# Display the combined DataFrame\n",
    "display(combined_results)\n",
    "\n",
    "# Perform one-way ANOVA for each metric\n",
    "metrics = [\"MSE\", \"MAE\", \"SSI\", \"MI\"]\n",
    "anova_results = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    anova_results[metric] = f_oneway(\n",
    "        [results.loc[\"Non-Deformable\", metric]],\n",
    "        results.loc[[\"Elastix\", \"VoxelMorph\"], metric].values\n",
    "    )\n",
    "\n",
    "# Display the ANOVA results\n",
    "for metric in metrics:\n",
    "    print(f\"ANOVA results for {metric}:\")\n",
    "    print(f\"F-statistic: {anova_results[metric].statistic}, p-value: {anova_results[metric].pvalue}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea822900",
   "metadata": {},
   "source": [
    "Lets interpret the results:\n",
    "- **F-statistic:** This value indicates the ratio of the variance between the groups to the variance within the groups. Higher values typically suggest a greater difference between the groups.\n",
    "- **p-value:** This value indicates the probability of observing the results if there were no actual differences between the groups. A common threshold for statistical significance is 0.05. If the p-value is below this threshold, we reject the null hypothesis and conclude that there is a statistically significant difference between the groups.\n",
    "\n",
    "Based on this ANOVA results (p-values all greater than 0.05 and low F-statistics, we conclude that there is no statistically significant difference in performance between the \"Non-Deformable\" method and the combined \"Elastic Methods\" (Elastix and VoxelMorph) for any of the metrics (MSE, MAE, SSI, MI). This suggests that, under the given conditions and data, the methods perform similarly in terms of the metrics evaluated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
